\chapter{\IfLanguageName{dutch}{Proof-of-Concept}{Proof-of-Concept}}%
\label{ch:proof-of-concept}

\section{Introductie}
Als PoC wordt er een mobiele applicatie te ontwikkeld. Deze applicatie is in staat om het gezicht te detecteren en er bepaalde kenmerken uit te halen om vervolgens de vermoeidheid te berekenen. De reden waarom er voor een mobiele applicatie gekozen is is heel simpel. Tegenwoordig gebruikt iedereen wel een houdertje om zijn/haar gsm in te stoppen tijdens het rijden. Vaak is dit ook op de bestuurder gericht. Doordat dan onrechtstreeks de camera vaak op de bestuurder gericht staat, kan de applicatie vervolgens het gezicht detecteren en zo de conclusie er uit halen of hij al dan niet vermoeid is. De applicatie maakt gebruik van MediaPipe zijn Face Landmark detectie en breidt zo uit op het uitvoeren van formules voor het berekenen van Eye Aspect Ratio en PERCLOS.

\section{Doelstellingen}
Het belangrijkste doel van deze applicatie is, om door vermoeidheid te detecteren, het voorkomen van het gevaar in de verkeersveiligheid. Er gebeuren dagelijks wel wat ongevallen, zowel overdag als 's nachts. Één van de oorzaken voor ongevallen is dat de reactietijd van de bestuurder vaak te traag is door vermoeidheid. Uiteraard spelen er nog andere factoren een grote rol, maar de grootste oorzaak van nachtelijke ongevallen is toch wel vaak de vermoeidheid. Deze PoC zal er proberen voor zorgen dat dit zo snel mogelijk gedetecteerd kan worden en de bestuurder kan 'wakker' maken, zodat hij/zij zelf kan inschatten of ze beter zouden slapen dan verder rijden.

\section{Technologieën}
\subsection{Kotlin}
De applicatie wordt ontwikkeld in Kotlin. Sinds de Google I/O conferentie is er aangekondigd dat Android development Kotlin-first is. Dit houdt in dat dit gezien wordt als de primaire programmeertaal om een Android applicatie te ontwikkelen. Het biedt namelijk veel voordelen. Enkele van deze voordelen zijn:
\begin{itemize}
    \item \textbf{Minder code met een grotere leesbaarheid}: Je hoeft minder tijd te besteden aan het schrijven van code en is vaak sneller te begrijpen.
    \item \textbf{Minder veelvoorkomende fouten}: Applicaties die gebouwd zijn met Kotlin hebben, volgens de interne gegevens van Google, 20\% minder kans om vast te lopen.
    \item \textbf{Kotlin wordt ondersteunt in Jetpack libraries}: De aanbevolen moderne toolkit om een native UI in Kotlin te bouwen is Jetpack Compose. KTX-extenties voegen ook taalfunctionaliteiten van Kotlin toe. Deze zijn coroutines, extension functions, lambdas en genoemde parameters aan andere Android libraries.
    \item \textbf{Ondersteuning voor multiplatform development}: Kotlin Multiplatform zorgt ervoor dat de development niet enkel voor Android gebruikers is, maar ook voor iOS, backend en webapplicaties.
\end{itemize}

\subsection{MediaPipe}
Om het gezicht te kunnen detecteren heb ik gekozen om MediaPipe Solutions te gebruiken. Dit biedt een variatie van verschillende libraries en tools om snel en eenvoudig kunstmatige intelligentie (AI) en machine learning (ML) technieken kunt toepassen in je applicaties. Solutions maakt deel uit van het open source project MediaPipe, dit houdt in dat je de oplossingen makkelijk kunt aanpassen aan je behoeftes en je ze kan gebruiken op meerdere platformen.
\subsubsection{Face Detection}
Ik had er eerst voor gekozen om MediaPipe zijn Face Detection solution te gebruiken. Dit zette mij in staat om het gezicht te kunnen detecteren en er vervolgens een vierkant rond te zetten. Nadien kon je de punten van interesse op zetten. Dit wil zeggen dat je kon aanduiden via een stip waar de ogen, mond, neus en oren zich bevonden. Al snel kwam ik er achter dat MediaPipe over een ander soort solution beschikte die mij meer kon helpen.
\subsubsection{Face Landmark Detection}
Deze solution zorgde ervoor dat ik een betere representatie had van het gezicht. Dit was in staat om zowel het gezicht te detecteren als de expressie. Het maakt gebruik van ML modellen die kunnen werken met zowel enkele afbeeldingen als een stroom van afbeeldingen. De output van deze solution was een virtuele avatar. Het maakte drie-dimensionele landmark punten op het gezicht. Bovendien gaf het ook een score van welke expressie het detecteert. 

\section{Implementatie}
De implementatie van de Proof of Concept (PoC) is onderverdeeld in enkele stappen. Elke stap draagt bij aan het geheel van de applicatie.

\subsection{Algemeen}
\subsubsection{Navigatie}
Eerst en vooral wordt er een navigatie voorzien. Hierin bevinden zich twee fragments. Zo heb je een \emph{PermissionFragment} en \emph{CameraFragment}. Dit gebeurt zodat de overschakeling van het ene fragment naar het andere fragment soepeler verloopt.
\begin{lstlisting}[language=Kotlin, caption=nav\_graph.xml]
    <navigation xmlns:android="http://schemas.android.com/apk/res/android"
        xmlns:app="http://schemas.android.com/apk/res-auto"
        android:id="@+id/nav_graph"
        app:startDestination="@id/permissions_fragment"> 
    </navigation>
\end{lstlisting}
\begin{itemize}
    \item \textbf{xmlns:android}: De XML-namespace voor standaard Android-attributen
    \item \textbf{xmlns:app}: De XML-namespace voor aangepaste attributen.
    \item \textbf{xmlns:id}: De unieke ID voor de layout.
    \item \textbf{xmlns:starDestination}: Het fragment dat als eerst moet worden weergegeven als de applicatie is opgestart.
\end{itemize}

\subsubsection{CoordinatorLayout}
Bepaalde fragments bestaan uit verschillende views. Deze views worden samen in een  \emph{CoordinatorLayout} gezet. Dit maakt coördinatie tussen de verschillende views mogelijk. Zo kun je meerdere views in één fragment hebben.
\begin{lstlisting}[language=Kotlin, caption=CoordinatorLayout]
    <androidx.coordinatorlayout.widget.CoordinatorLayout
        xmlns:android="http://schemas.android.com/apk/res/android"
        xmlns:app="http://schemas.android.com/apk/res-auto"
        android:id="@+id/camera_container"
        android:layout_width="match_parent"
        android:layout_height="match_parent">
    </androidx.coordinatorlayout.widget.CoordinatorLayout>
\end{lstlisting}

\begin{itemize}
    \item \textbf{xmlns:android}: De XML-namespace voor standaard Android-attributen
    \item \textbf{xmlns:app}: De XML-namespace voor aangepaste attributen.
    \item \textbf{xmlns:id}: De unieke ID voor de layout.
    \item \textbf{xmlns:layout\_width}: Breedte van de layout. Deze staat ingesteld op \emph{match\_parent} om de volledige breedte van het scherm te gebruiken.
    \item \textbf{xmlns:layout\_height}: Hoogte van de layout. Deze staat ingesteld op \emph{match\_parent} om de volledige hoogte van het scherm te gebruiken.
\end{itemize}


\subsection{Camera}
De implementatie van de PoC begint met het initialiseren van de camera. De camera is een cruciaal onderdeel van de applicatie omdat deze de input levert voor de verdere verwerking en analyse. Zonder een goed werkende camera kan de applicatie niet functioneren zoals bedoeld.

Voor deze applicatie maak ik gebruik van de CameraX bibiliotheek, die een eenvoudige en consistente API biedt voor cameratoegang en -beheer voor verschillende Android apparaten. In deze sectie toon ik even aan hoe het gebruik van de camera tot stand is gekomen.

\subsubsection{PermissionFragment}
Uiteraard heb ik toestemming nodig van de gebruiker om de camera te mogen gebruiken. Hiervoor is er een \emph{PermissionFragment} voorzien. Dit toont zich ook in de navigatie. Dit ziet er als volgt uit:
\begin{lstlisting}[language=Kotlin, caption=PermissionFragment]
    <fragment
    android:id="@+id/permissions_fragment"
    android:name="com.example.bach_poc.fragments.PermissionsFragment"
    android:label="PermissionsFragment">
    
    <action
    android:id="@+id/action_permissions_to_camera"
    app:destination="@id/camera_fragment"
    app:popUpTo="@id/permissions_fragment"
    app:popUpToInclusive="true" />
    </fragment>
\end{lstlisting}
De belangrijkste attributen van de fragment tag zijn \emph{android:id} en \emph{android:name}. Het eerste attribuut is de unieke ID van het fragment binnenin de navigatie. Deze ID wordt dus ook gebruikt om naar dit fragment te verwijzen bij navigatieacties. Het tweede attribuut is de naam van de klasse van het fragment. In dit geval zal er de klasse \emph{PermissionsFragment} gebruikt worden. Binnen in de action tag zijn er nog andere attributen. Zo heb je \emph{app:destination}, wat de bestemming van de navigatieactie is, \emph{app:popUpTo}, wat specificeert tot welk punt de backstack gewist zal worden en \emph{app:popUpToInclusive}, wat er voor zorgt dat 'permissions\_fragment' ook uit de backstack gewist zal worden.

Nu zal er wat uitleg gegeven worden over wat er precies in de klasse \emph{PermissionsFragment.kt} zit. Om aan te tonen dat deze klasse een fragment is, zal het dan ook overerven van \emph{Fragment}. Deze heeft een methode \emph{onCreate} die wordt opgeroepen om de gebruikersinterface te creëren.
\begin{lstlisting}[language=Kotlin,caption=override fun onCreate(savedInstanceState: Bundle?) in PermissionsFragment.kt, label={lst:onCreatePermissions}]
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        when {
            ContextCompat.checkSelfPermission(
            requireContext(),
            Manifest.permission.CAMERA
            ) ==
            PackageManager.PERMISSION_GRANTED -> {
                navigateToCamera()
            }
            else -> {
                requestPermissionLauncher.launch(Manifest.permission.CAMERA)
            }
        }
    }
\end{lstlisting}
Zodra de fragment gemaakt wordt, wordt de basisimplementatie van \emph{Fragment} uitgevoerd. Vervolgens wordt er gecontroleerd of de benodigde permissie verleend is. Dit gebeurt in de \emph{when} conditie. Die gaat de permissie voor de camera ophalen en controleren of hij al dan niet verleend is, dit gebeurt door de conditie in de accolades na \emph{when}. Dit haalt de context van het fragment op inclusief \emph{Manifest.permission.CAMERA}, wat de constante is die de camera-permissie vertegenwoordigt. Vervolgens kijkt hij of de permissie wel degelijk toegelaten is (\emph{PackageManager.PERMISSION\_GRANTED}). Indien de permissie niet is toegelaten, zal hij de popup lanceren die vraagt naar de toestemming.

\begin{lstlisting}[language=Kotlin, caption=requestPermissionLauncher in PermissionsFragment.kt, label={lst:requestPermissionLauncher}]
    private val requestPermissionLauncher =
    registerForActivityResult(ActivityResultContracts.RequestPermission()) { 
        isGranted: Boolean ->
            if (isGranted) {
                Toast.makeText(
                context,
                "Permission request granted",
                Toast.LENGTH_LONG
                ).show()
                navigateToCamera()
            } else {
                Toast.makeText(
                context,
                "Permission request denied",
                Toast.LENGTH_LONG
                ).show()
            }
        }
\end{lstlisting}
Hierbij wordt er een \emph{ActivityResultLauncher} gelanceerd. Die zal de permissieaanvraag uitvoeren. De callbackfunctie \emph{isGranted} zal de boolean opslaan en zal vervolgens een passend bericht aan tonen als de aanvraag geaccepteerd of afgewezen is.

\begin{lstlisting}[language=Kotlin, caption=navigateToCamera() in PermissionsFragment.kt]
    private fun navigateToCamera() {
        lifecycleScope.launch {
            lifecycle.repeatOnLifecycle(Lifecycle.State.STARTED) {
                Navigation.findNavController(
                requireActivity(),
                R.id.fragment_container
                )
                .navigate(R.id.camera_fragment)
            }
        }
    }
    
\end{lstlisting}
Deze functie wordt uitgevoerd indien de permissie van de camera wel toegelaten is. Er wordt een nieuwe coroutine gestart door \emph{lifecycleScope.launch}. Een coroutine is gekoppeld aan de levenscyclus van een fragment, in dit geval van \emph{PermissionsFragment.kt}. Dit wil zeggen dat het dus geannuleerd wordt indien het fragment of de activiteit 'vernietigd' wordt. Door de regel \emph{lifecycle.repeatOnLifecycle(Lifecycle.State.STARTED)} zal de code binnenin dat codeblock enkel uitgevoerd worden wanneer het fragment in de toestand 'STARTED' is. Vervolgens wordt er genavigeerd naar het fragment met id \emph{camera\_fragment} vanuit de huidige fragment.

\begin{lstlisting}[language=Kotlin, caption=companion object van PermissionsFragment.kt, label={lst:companionPermssion}]
private val PERMISSIONS_REQUIRED = arrayOf(Manifest.permission.CAMERA)

companion object {
    fun hasPermissions(context: Context) =
    PERMISSIONS_REQUIRED.all {
        ContextCompat.checkSelfPermission(
        context,
        it
        ) == PackageManager.PERMISSION_GRANTED
    }
}
\end{lstlisting}
Dit is het \emph{companion object} van \emph{PermissionsFragment}. Deze functie is gekoppeld aan de definitie van de klasse, wat dus wilt zeggen is dat het de functie zal uitvoeren bij het instantiëren. In dit geval return de functie een \emph{boolean} om te kijken of de permissies die in \emph{PERMISSIONS\_REQUIRED} staan zijn toegelaten of niet.

\subsubsection{CameraFragment}
Net zoals \emph{PermissionFragment} erft \emph{CameraFragment} ook over van \emph{Fragment}, maar ook van \emph{FaceLandmarkerHelper.LandmarkListener}, waar nog verdere uitleg over zal gegeven worden.
In tegenstelling tot de \emph{PermissionsFragment} beschikt deze fragment over enkele variabelen die in het geheel van de klasse worden gebruikt. Deze variabelen zullen vermeld worden bij de methodes die ze gebruiken.

\begin{lstlisting}[language=Kotlin, caption=onCreateView in CameraFragment.kt, label={lst:onCreateViewCameraFragment}]
    private var _fragmentCameraBinding: FragmentCameraBinding? = null
    private val fragmentCameraBinding get() = _fragmentCameraBinding!!

    override fun onCreateView(
        inflater: LayoutInflater,
        container: ViewGroup?,
        savedInstanceState: Bundle?): View?
        {
            _fragmentCameraBinding = FragmentCameraBinding
                                .inflate(inflater,container,false)
    
            return fragmentCameraBinding.root
        }
\end{lstlisting}
Deze methode wordt uitgevoerd terwijl de view wordt aangemaakt. De variabele \emph{\_fragmentCameraBinding} is een gegenereerde klasse dat overeenkomt met de layout file van het fragment (\emph{fragment\_camera.xml}). Nadien wordt de variabele \emph{fragmentCameraBinding} gebruikt als een \emph{get()} functie om het fragment effectief te verkrijgen. De methode \emph{onCreateView} gaat de view van de camera maken. Dit doet hij door het fragment 'op te blazen' met de view, wat dus wilt zeggen dat de view in de layout wordt gestopt. Daarna wordt de top-level view gereturned (\emph{fragmentCameraBinding.root}).

\begin{lstlisting}[language=Kotlin, caption=onViewCreated in CameraFragment.kt, label={lst:onViewCreatedCameraFragment}]
    private lateinit var backgroundExecutor: ExecutorService
    
     private val faceBlendshapesResultAdapter by lazy {
        FaceBlendshapesResultAdapter()
    }
    
    
    @SuppressLint("MissingPermission")
    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {
        super.onViewCreated(view, savedInstanceState)
        
        with(fragmentCameraBinding.recyclerviewResults) {
            layoutManager = LinearLayoutManager(requireContext())
            adapter = faceBlendshapesResultAdapter
        }
        
        backgroundExecutor = Executors.newSingleThreadExecutor()
        
        fragmentCameraBinding.viewFinder.post {
            setUpCamera()
        }
        
        backgroundExecutor.execute {
            faceLandmarkerHelper = FaceLandmarkerHelper(
            context = requireContext(),
            minFaceDetectionConfidence = 0.5F,
            minFacePresenceConfidence = 0.5F,
            minFaceTrackingConfidence = 0.5F,
            maxNumFaces = 1,
            currentDelegate = 0,
            faceLandmarkerHelperListener = this
            )
        }
    }
\end{lstlisting}
Deze methode maakt gebruik van een \emph{ExecutorService}. Dit is een extensie van \emph{Executor}. Deze service zorgt er voor dat er asynchrone taken uitgevoerd kunnen worden. De \emph{@SuppressLint(`MissingPermission`)} is een annotatie dat de lint-waarschuwing onderdrukt voor ontbrekende toestemmingen, deze zijn namelijk al behandeld in \emph{PermissionFragment}. Nadat de view gemaakt is, wordt dit allemaal uitgevoerd.

De \emph{RecyclerView} van de layout wordt eerst opgehaald in het \emph{with} blok, dit zal later nog uitgelegd worden. De \emph{layoutManager} is hetgeen wat de layout zal beheren, de adapter wordt ingesteld als een hulpklasse van \emph{FaceBlendshapesResultAdapter}, dit is verantwoordelijk voor het weergeven van de resultaten van de gezichtsanalyse en zal nog verder uitgelegd worden.

Nadien wordt er een \emph{backgroundExecutor} gemaakt. Dit is een thread executor dat verdere taken op de achtergrond zal uitvoeren. Na al deze elementen geïnitialiseerd zijn zal de camera zich klaarmaken voor gebruik. Dit gebeurt echter enkel als de view gemaakt is en de \emph{viewFinder} beschikbaar is. Tenslotte zal de \emph{backgroundExecutor} een \emph{FaceLandmarkerHelper} maken op de achtergrond. De parameters van deze klasse worden later besproken.

\begin{lstlisting}[language=Kotlin, caption=setUpCamera in CameraFragment.kt, label={lst:setUpCamera}]
    private var cameraProvider: ProcessCameraProvider? = null
    
    private fun setUpCamera() {
        val cameraProviderFuture = ProcessCameraProvider
                                        .getInstance(requireContext())
        cameraProviderFuture.addListener(
        {
            cameraProvider = cameraProviderFuture.get()
            
            bindCameraUseCases()
        },
        ContextCompat.getMainExecutor(requireContext())
        )
    }
\end{lstlisting}
Hier wordt de camera klaargemaakt voor gebruik. Deze methode instantieert de variabele \emph{cameraProvider}. Dit is een singleton wat gebruikt wordt om de levenscyclus van de camera aan te koppelen. Binnenin de functie wordt de instantie van de huidige context van de applicatie gedeclareerd aan een nieuwe variabele. Vervolgens wordt er een \emph{listener} toegevoegd aan de instantie.

De code binnenin de accolades van de \emph{addListener} wordt uitgevoerd zodra de algemene executor van de context wordt opgehaald in \emph{ContextCompat.getMainExecutor(requireContext())}. In die blok van code wordt de \emph{cameraProvider} gedeclareerd en worden de usecases van de camera ingesteld (\emph{bindCameraUseCases}).

\begin{lstlisting}[language=Kotlin, caption=bindCameraUseCases in CameraFragment.kt, label{lst:bindCameraUseCases}]
    private var preview: Preview? = null
    private var imageAnalyzer: ImageAnalysis? = null
    private var camera: Camera? = null
    private fun bindCameraUseCases() {
     
        val cameraProvider = cameraProvider?:
                            throw IllegalStateException("Camera initialization failed.")
        
     
        val cameraSelector = CameraSelector.Builder()
        .requireLensFacing(CameraSelector.LENS_FACING_FRONT).build()
        
        val screenSize = Size(640, 480)
        val resolutionSelector = ResolutionSelector.Builder()
                            .setResolutionStrategy(ResolutionStrategy(screenSize,
                            ResolutionStrategy.FALLBACK_RULE_NONE)).build()
        
        preview = Preview.Builder()
        .setResolutionSelector(resolutionSelector)
        .build()
        
        
        imageAnalyzer = ImageAnalysis.Builder()
        .setResolutionSelector(resolutionSelector)
        .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
        .setOutputImageFormat(OUTPUT_IMAGE_FORMAT_RGBA_8888)
        .build()
        .also {
            it.setAnalyzer(backgroundExecutor) { image ->
                detectFace(image)
            }
        }
        
        
        cameraProvider.unbindAll()
        
        try {
            camera = cameraProvider.bindToLifecycle(
            this,
            cameraSelector,
            preview,
            imageAnalyzer
            )
            
            preview?.setSurfaceProvider(fragmentCameraBinding
                                            .viewFinder.surfaceProvider)
        } catch (exc: Exception) {
            Log.e(TAG, "Use case binding failed", exc)
        }
    }
\end{lstlisting} 
Deze methode heeft nood aan wat extra variabelen. We maken gebruik van \emph{Camera}, \emph{Preview} en \emph{ImageAnalyzer}. Eerst en vooral wordt er gekeken of de \emph{cameraProvider} al dan niet bestaat. Indien hij niet bestaat, zal er een \emph{IllegalStateException} gegooid worden. 

Vervolgens wordt de richting van de camera ingesteld. Dit gebeurt door \emph{CameraSelector.Builder().requireLensFacing(CameraSelector.LENS\_FACING\_FRONT).build()}. Dit roept de interface van de camera op en laat zo weten dat we behoefte hebben aan het gebruik van de selfie-camera.

Voor dat de preview van de camera gebuild wordt, zodat we ons zelf kunnen zien, is er nood aan het instellen van een \emph{ResolutionSelector}. Dit stelt de resolutie van de \emph{UseCase} in. De schermgrootte wordt ingesteld op 640x480 pixels en wordt vervolgens gebruikt om een \emph{ResolutionStrategy} in te stellen. Tenslotte kan zo de \emph{ResolutionSelector} van de \emph{UseCase} gebuild worden. 
 
Nadat we een resolutie hebben, kan de \emph{Preview} gemaakt worden. De \emph{Preview} zorgt er voor dat we de camera kunnen bekijken die gebruikt wordt, we kunnen onszelf dus zien. 

Nadien wordt er een \emph{ImageAnalyzer} gedeclareerd. Dit zorgt ervoor dat we het beeld kunnen analyseren dat wordt opgenomen. Deze analyzer gebruikt dezelfde instellingen voor de resolutie als die van de camera. Dit doe ik zodat het camerabeeld direct geanalyseerd kan worden en niet eerst nog omgezet moet worden naar eventueel een andere resolutie voor de analyse. De \emph{.setBackpressureStrategy} wordt ingesteld op \emph{STRATEGY\_KEEP\_ONLY\_LATEST}. Dit wilt zeggen dat enkel de laatste \emph{image} wordt doorgestuurd naar de analyzer, de voorgaande worden dus gedropt. Dit is noodzakelijk doordat sommige frames sneller verstuurd worden dan dat ze geanalyseerd worden. De analyzer zal de images die hij moet analyseren ontvangen in \emph{RGBA\_8888} formaat. Het wordt dus verstuurd met een Rood, Groen, Blauw en Alpha waarde. De 8888 staat voor het aantal bits dat gebruikt wordt om elke waarde te coderen. Dit is het standaardformaat. Tenslotte wordt de analyzer gebuild en zal er een analyzer voor de \emph{backgroundExecutor} ingesteld worden, die zal op de achtergrond het gezicht detecteren van de image.

Voordat er nieuwe use cases aan de \emph{cameraProvider} gebonden kunnen worden, moeten ze eerst ontbonden worden. Dit gebeurt door \emph{.unbindAll()}. Dit zorgt er voor dat, indien er andere fragments bestaan die ook bindingen hebben aan de provider, al de bindingen zich ontbinden.

Als laatste stap worden de 3 use cases: \emph{CameraSelector}, \emph{Preview} en \emph{ImageAnalysis} gebonden aan de levenscyclus van de cameraprovider en het oppervlak van de \emph{viewFinder} zich aan de preview binden. Indien dat niet lukt, zal er zich een error voordoen.

\begin{lstlisting}[language=Kotlin, caption=detectFace in CameraFragment.kt, label={lst:detectFace}]
    
    private fun detectFace(imageProxy: ImageProxy){
        faceLandmarkerHelper.detectLivestream(
        imageProxy = imageProxy
        )
    }
    
\end{lstlisting}
De methode \emph{detectFace} gaat het beeld van de \emph{ImageAnalyzer} nemen en gaat de methode \emph{detectLiveStream(ImageProxy imageProxi)} oproepen die in de \emph{FaceLandmarkerHelper} staat.

We zien dat \emph{CameraFragment} overerft van \emph{FaceLandMarkerHelper.Listener}. Dit is een interface die in \emph{FaceLandmarkerHelper} staat. Deze beschikt over 3 methodes: \emph{onError}, \emph{onEmpty} en \emph{onResults}.
\begin{lstlisting}[language=Kotlin, caption=onError in CameraFragment.kt, label={lst:onError}]
override fun onError(error: String, errorCode: Int) {
    activity?.runOnUiThread {
        Toast.makeText(requireContext(), error, Toast.LENGTH_SHORT).show()
        faceBlendshapesResultAdapter.updateResults(null)
        faceBlendshapesResultAdapter.notifyDataSetChanged()
    }
}
\end{lstlisting}
Dit zorgt ervoor dat de gebruiker op de hoogte wordt gebracht indien er zich een fout voordoet. Als er effectief een fout voorkomt, zullen de resultaten op \emph{null} worden ingesteld en wordt er aan \emph{faceBlendshapesResultdAdapter} gecommuniceerd dat de dataset veranderd is.

\begin{lstlisting}[language=Kotlin, caption=onEmpty in CameraFragment.kt, label={lst:onEmpty}]
    override fun onEmpty() {
        fragmentCameraBinding.overlay.clear()
        activity?.runOnUiThread {
            faceBlendshapesResultAdapter.updateResults(null)
            faceBlendshapesResultAdapter.notifyDataSetChanged()
        }
    }
\end{lstlisting}
Deze methode zal er voor zorgen dat de applicatie reageert als er geen gezicht wordt gedetecteerd. Hij wist ook de mogelijke visuele overlay en zal ook de resultaten op \emph{null} gezet worden.

\begin{lstlisting}[language=Kotlin, caption=onResults in CameraFragment.kt, label={lst:onResults}]
    override fun onResults(resultBundle: FaceLandmarkerHelper.ResultBundle) {
        activity?.runOnUiThread {
            if (_fragmentCameraBinding != null) {
                if (fragmentCameraBinding.recyclerviewResults
                                        .scrollState != SCROLL_STATE_DRAGGING)
                {
                    faceBlendshapesResultAdapter
                                    .updateResults(resultBundle.result)
                    faceBlendshapesResultAdapter.notifyDataSetChanged()
                }
                
                fragmentCameraBinding.overlay.setResults(
                resultBundle.result,
                resultBundle.inputImageHeight,
                resultBundle.inputImageWidth,
                )
                fragmentCameraBinding.overlay.invalidate()
            }
        }
    }
\end{lstlisting}
Deze methode zal de resultaten van de camera doorgeven aar \emph{faceBlendshapesResultAdapater} indien er niet gescrolled wordt op de applicatie. Vervolgens zullen de resultaten ook doorgegeven worden aan het overlay component. Dit component zal tonen waar het gezicht gedetecteerd is.

\subsection{Face Landmark Detectie}
Om het gezicht te kunnen detecteren en weer te geven aan de gebruiker, zijn er enkele hulpklassen gemaakt. Deze klassen zorgen ervoor dat het doorgegeven beeld gebruikt kan worden om daarop het gezicht te detecteren en nadien een visueel beeld kan returnen van de landmarks.

\subsubsection{FaceLandmarkerHelper}
Deze klasse zal het gezicht op het beeld detecteren. Dit kan gezien worden als de basis van de gezichtsdetectie. Nu zullen de methoden die gebruikt zijn in deze klasse uitgelegd worden. Voor dat we de verder duiken in deze klasse, zal er eerst wat informatie gegeven worden over het \emph{companion object} zodat we toegang hebben tot verschillende constanten bij het aanmaken van deze klasse.

\begin{lstlisting}[language=Kotlin, caption=companion object van FaceLandmarkerHelper.kt, label={lst:companionFaceLandmarkerHelper}]
    companion object {
        const val TAG = "FaceLandmarkerHelper"
        private const val MP_FACE_LANDMARKER_TASK = "face_landmarker.task"
        
        const val DELEGATE_CPU = 0
        const val DELEGATE_GPU = 1
        const val DEFAULT_FACE_DETECTION_CONFIDENCE = 0.5F
        const val DEFAULT_FACE_TRACKING_CONFIDENCE = 0.5F
        const val DEFAULT_FACE_PRESENCE_CONFIDENCE = 0.5F
        const val DEFAULT_NUM_FACES = 1
        const val OTHER_ERROR = 0
        const val GPU_ERROR = 1
        
    }
\end{lstlisting}
De constante \emph{TAG} wordt vooral gebruikt bij het loggen. Indien er zich dan een fout voordoet, kan er in de logs gekeken worden in welke klasse deze error zich voordoet. \emph{MP\_FACE\_LANDMARKER\_TASK} is de constante dat een relatie legt aan de \emph{task} die het detecteren zal volbrengen. Deze taak is in de \emph{assets} folder gezet. De overige constanten die ingesteld staan als parameters om de taak aan te passen.

\begin{lstlisting}[language=Kotlin, caption=interface LandmarkListener, label={lst:interfaceLandmarkListener}]
     interface LandmarkListener {
        fun onError(error: String, errorCode: Int = OTHER_ERROR)
        fun onResults(resultBundle: ResultBundle)
        fun onEmpty() {}
    }
\end{lstlisting}
Dit is de interface \emph{LandmarkListener}. Doordat we \emph{RunningMode.LIVE\_STREAM} gebruiken, hebben we nood aan deze interface. Deze methodes zijn uitgeschreven in \emph{CameraFragment}. Dit houdt dus in dat dit eigenlijk een \emph{LandmarkListener} is en dus de resultaten asynchroon zal ontvangen.


\begin{lstlisting}[language=Kotlin, caption=init() en setupFaceLandmarker() in FaceLandmarkerHelper.kt, label={lst:setupFaceLandmarker}]
    init {
        setupFaceLandmarker()
    }
    
    private var faceLandmarker: FaceLandmarker? = null
    
    fun setupFaceLandmarker() {
        val baseOptionBuilder = BaseOptions.builder()
        
        when (currentDelegate) {
            DELEGATE_CPU -> {
                baseOptionBuilder.setDelegate(Delegate.CPU)
            }
            DELEGATE_GPU -> {
                baseOptionBuilder.setDelegate(Delegate.GPU)
            }
        }
        
        baseOptionBuilder.setModelAssetPath(MP_FACE_LANDMARKER_TASK)
        
        if (faceLandmarkerHelperListener == null) {
            throw IllegalStateException(
                    "faceLandmarkerHelperListener must be set when runningMode is LIVE_STREAM")
        }
        
        try {
            val baseOptions = baseOptionBuilder.build()
            val optionsBuilder =
            FaceLandmarker.FaceLandmarkerOptions.builder()
            .setBaseOptions(baseOptions)
            .setMinFaceDetectionConfidence(minFaceDetectionConfidence)
            .setMinTrackingConfidence(minFaceTrackingConfidence)
            .setMinFacePresenceConfidence(minFacePresenceConfidence)
            .setNumFaces(maxNumFaces)
            .setOutputFaceBlendshapes(true)
            .setRunningMode(RunningMode.LIVE_STREAM)
            .setResultListener(this::returnLivestreamResult)
            .setErrorListener(this::returnLivestreamError)
            
            val options = optionsBuilder.build()
            faceLandmarker = FaceLandmarker.createFromOptions(context,options)
        } catch (e: IllegalStateException) {
            faceLandmarkerHelperListener?.onError(
            "Face Landmarker failed to initialize, See error logs for details"
            )
            Log.e(TAG, "MediaPipe failed to load the task with error" + e.message)
        }
        catch (e: RuntimeException) {
            faceLandmarkerHelperListener?.onError(
            "Face Landmarker failed to initialize. See error logs for " +
            "details", GPU_ERROR
            )
            Log.e(TAG, "Face Landmarker failed to load model with error: " + 
                                                                        e.message)
        }
    }
\end{lstlisting}

De klasse wordt geïnitialiseerd door een \emph{init()} methode. In die methode wordt \emph{setupFaceLandmarker} opgeroepen. Het grote beeld van deze methode is het gereedmaken van het gebruik van de \emph{Face Landmark detectie}.

Eerst en vooral wordt er een \emph{BaseOptions} ingesteld. Die opties vormen de basis van de taak. Het eerste \emph{if} statement gaat selecteren wat de taak gaat uitvoeren. Dat kan zowel de CPU als de GPU zijn, afhankelijk van hoe het is ingesteld. In het geval van deze Proof of Concept is het de CPU. Doordat onze applicatie niet zo krachtig is, heeft het geen impact op de performance, je zou dus evengoed de GPU kunnen kiezen. De \emph{currentDelegate} wordt meegegeven als parameter en staat ingesteld op \textbf{\emph{var currentDelegate: Int = DELEGATE\_CPU}}. Er kan dus een andere delegate meegegeven worden indien dat gewenst is, anders zal de CPU gebruikt worden.

Vervolgens wordt het pad van de taak ingesteld. Dit is de constante
\emph{MP\_FACE\_LANDMARKER\_TASK} die we in het \emph{companion object} gedeclareerd hebben. Zo weet de \emph{BaseOptions} welke taak hij moet uitvoeren, in dit geval \emph{face\_landmarker.task}.

Het \emph{if} statement nadat het \emph{assetPath} is ingesteld, kijkt of er wel een \emph{faceLandmarkerHelperListener} is. Ook dit is een parameter van deze klasse \textbf{\emph{var faceLandmarkerHelperListener: LandmarkListener? = null}}. Een \emph{LandmarkListener} is noodzakelijk wanneer de \emph{RUNNING.MODE} wordt ingesteld op livestream. Zo krijgen we een continue detectie en analyse. Eerder gezien in de \emph{onViewCreated} wordt \emph{CameraFragment} meegegeven als \emph{LandmarkListener}. Zo weet de taak of er nieuwe beelden binnenkomen, wat in dit geval constant is.

Tenslotte wordt er een \emph{try catch} uitgevoerd. In het try gedeelte van de code wordt eerst en vooral de \emph{baseOptions} gebuild. Nadien wordt de \emph{optionsBuilder} samengesteld. Dit gaat de \emph{FaceLandmarker} builden. Daar horen enkele zaken bij:
\begin{itemize}
    \item \textbf{setBaseOptions}: De basisopties van de taak. In dit geval de delegate dat gebruikt wordt.
    \item \textbf{setMinFaceDetectionConfidence(minFaceDetectionConfidence)}: De minimum zelfzekerheid score voor het gezicht dat moet voldoen om het detecteren een succes te noemen. Staat ingesteld op 0.5.
    \item \textbf{setMinTrackingConfidence(minFaceTrackingConfidence)}: De minimum zelfzekerheid score voor het traceren van het gezicht. Staat ingesteld op 0.5.
    \item \textbf{setMinFacePrsenceConfidence(minFacePresenseConfidence)}: De minimum zelfzekerheid score van het gezicht dat aanwezig is. Staat ingesteld op 0.5.
    \item \textbf{setNumFaces(maxNumFaces)}: Het maximum aantal gezichten dat gedetecteerd kan worden. Staat ingesteld op 1.
    \item \textbf{setOutputFaceBlendshapes(true)}: Dit zorgt ervoor dat het resultaat gebruikt kan worden voor het renderen van een 3D model.
    \item \textbf{setRunningMode(RunningMode.LIVE\_STREAM)}: RunningMode van de taak. Sinds het \emph{LIVE\_STREAM} is, moet er een \emph{resultListener} worden meegegeven om de resultaten asynchroon te ontvangen.
    \item \textbf{setResultListener(this::returnLiveStreamResult)}: Zorgt ervoor dat de resultaten asynchroon ontvangen worden. Kan enkel gebruikt worden als de RunningMode ingesteld staat op \emph{LIVE\_STREAM}
    \item \textbf{setErrorListener(this::returnLiveStreamError)}: Optionele \emph{listener} die errors gaat afhandelen.
\end{itemize}
Al de parameters die hierboven zijn gebruikt, staan in het \emph{companion object}. Nadat de taak succesvol geconfigureerd is, zal hij gebouwd worden. Indien er ergens iets mis loopt in het \emph{try} blok, zijn er \emph{catch} blokken voorzien om de errors te melden.

\begin{lstlisting}[language=Kotlin, caption=returnLiveStreamResult in FaceLandmarkerHelper.kt, label={lst:returnLiveStreamResult}]
    private fun returnLivestreamResult(
    faceLandmarkerResult: FaceLandmarkerResult,
    mpImage: MPImage
    ) {
        if (faceLandmarkerResult.faceLandmarks().size > 0) {
            val finishTimeMs = SystemClock.uptimeMillis()
            val inferenceTime = finishTimeMs - faceLandmarkerResult.timestampMs()
            
            faceLandmarkerHelperListener?.onResults(
            ResultBundle(
            faceLandmarkerResult,
            inferenceTime,
            mpImage.height,
            mpImage.width
            )
            )
        }
        else {
            faceLandmarkerHelperListener?.onEmpty()
        }
    }
    
    data class ResultBundle(
        val result: FaceLandmarkerResult,
        val inferenceTime: Long,
        val inputImageHeight: Int,
        val inputImageWidth: Int,
    )
\end{lstlisting}
Deze methode zal het resultaat teruggeven van de detectie. Deze functie heeft als parameters \emph{FaceLandmarkerResult} en \emph{MPImage}. Het doel van deze functie is het controleren of er daadwerkelijk gezichten gedetecteerd zijn of niet. Indien dit het geval is, zal er berekend worden hoelang het geduurd heeft tot de resultaten. Dit gebeurt door \emph{inferenceTime}. Het is de tijd sinds de applicatie is opgestart - de tijd dat het geduurd heeft om een gezicht te detecteren. Tenslotte wordt er een \emph{ResultBundle} gemaakt. Daarin zit het resultaat, de tijd en de hoogte en breedte van de camerabeelden. Als er geen gezichten gedetecteerd zijn, zal de \emph{onEmpty()} functie opgeroepen worden.

\begin{lstlisting}[language=Kotlin, caption=returnLiveStreamError in FaceLandmarkerHelper.kt, label={lst:returnLivestreamError}]
    private fun returnLivestreamError(runtimeException: RuntimeException) {
        faceLandmarkerHelperListener?.onError(
        runtimeException.message ?: "An unknown error has occurred"
        )
    }
\end{lstlisting}
Dit is de functie die een error zal werpen indien er zich een \emph{RuntimeException} voordoet.

\begin{lstlisting}[language=Kotlin, caption=detectLivestream in FaceLandmarkerHelper.kt, label={lst:detectLivestream}]
    fun detectLivestream(
    imageProxy: ImageProxy
    ) {
        val frameTime = SystemClock.uptimeMillis()
        
        val bitmapBuffer = Bitmap.createBitmap(
        imageProxy.width,
        imageProxy.height,
        Bitmap.Config.ARGB_8888
        )
        imageProxy.use {bitmapBuffer.copyPixelsFromBuffer(
                                                    imageProxy.planes[0].buffer)}
        imageProxy.close()
        
        val matrix = Matrix().apply {
            postRotate(imageProxy.imageInfo.rotationDegrees.toFloat())
            postScale(-1f, 1f, imageProxy.width.toFloat(),
                                    imageProxy.height.toFloat())
        }
        
        val rotatedBitmap = Bitmap.createBitmap(bitmapBuffer, 0, 0,
                                  bitmapBuffer.width, bitmapBuffer.height,
                                                                 matrix, true)
        
        val mpImage = BitmapImageBuilder(rotatedBitmap).build()
        
        detectAsync(mpImage, frameTime)
    }
\end{lstlisting}
Het algemeen doel van deze functie is het omzetten van een camera frame (\emph{ImageProxy)} naar een formaat dat MediaPipe kan analyseren (\emph{MPImage}).

Er wordt eerst een \emph{Bitmap} gemaakt. De bitmap heeft dezelfde breedte en hoogte als het beeld en zal worden geconfigureerd in \emph{ARGB\_8888}. Dit is dezelfde configuratie als in \emph{CameraFragment}. Daarna worden de pixels van de \emph{ImageProxy} gekopieerd naar de des betreffende \emph{bitmapBuffer}. De \emph{use} blok zorgt er voor dat dit mogelijk is en zorgt er ook voor dat we de \emph{ImageProxy} kunnen sluiten achteraf.

Vervolgens wordt er een \emph{Matrix} object aangemaakt. Dit is optioneel en zorgt er voor dat de bitmap geroteerd kan worden. Hetgeen wat er voor zorgt dat de bitmap corrigeert voor de rotatie van de camera is \emph{postRotate}. Die gaat de bitmap roteren met het aantal graden dat is opgeslagen in \emph{imageProxy.imageInfo.rotationDegrees}. Terwijl \emph{postScale} er voor gaat zorgen dat de bitmap over de Y-as gespiegeld wordt. Dit komt omdat de beelden van de frontcamera vaak gespiegeld zijn.

Uiteindelijk wordt er een geroteerde bitmap gemaakt met de huidige \emph{bitmapBuffer} en \emph{Matrix} zodat het detecteren mooi op het gezicht zelf valt.

Tenslotte wordt de bitmap omgezet in een \emph{MPImage} zodat de \emph{FaceLandmarker} het kan analyseren.

\begin{lstlisting}[language=Kotlin, caption=detectAsync in FaceLandmarkerHelper.kt, label={lst:detectAsync}]
    private fun detectAsync(mpImage: MPImage, frameTime: Long) {
        faceLandmarker?.detectAsync(mpImage,frameTime)
    }
\end{lstlisting}
Deze methode roept de \emph{detectAsync} methode op van de \emph{FaceLandmarker} klasse. Dit is een functie van MediaPipe zelf.

\subsection{Overlay}
Om een visueel beeld te krijgen van het gedetecteerd gezicht, is er een overlay. Deze overlay bestaat uit een 3D model van het gezicht en een lijst van categorieën. Deze categorieën tonen aan wat het precies detecteert. Dit varieert van \emph{eyeBlinkleft} tot \emph{jawOpen}. We zullen eerst kijken hoe deze categorieën verkregen worden.

\subsubsection{FaceBlendshapesResultAdapter}
Dit is de klasse die er voor zal zorgen dat de oriëntatiepunten van de ogen opgehaald kunnen worden en vervolgens de Eye Aspect Ratio en PERCLOS berekend. Deze klasse erft over van \emph{RecyclerView.Adapter<FaceBlendshapesResultAdapter.ViewHolder>}. Dit wordt dus een adapter voor een \emph{RecyclerView}. Het gebruikt een aangepaste \emph{ViewHolder} klasse. Hierdoor kunnen we de resultaten weergeven.
\begin{lstlisting}[language=Kotlin, caption=ViewHolder klasse in FaceBlendshapesResultAdapter, label={lst:ViewHolder}]
    override fun onBindViewHolder(holder: ViewHolder, position: Int) {
        holder.bind(perclos)
    }
    
    inner class ViewHolder(private val binding: FaceBlendshapesResultBinding):
                                      RecyclerView.ViewHolder(binding.root) {
        fun bind(score: Double){
            with(binding) {
                tvLabel.text = "PERCLOS"
                tvScore.text = String.format(Locale.ENGLISH, "%.2f%%", score)
                if (score < 20) tvState.text = "Alert"
                else if (score > 20 && score < 30)  tvState.text = "Drowsy"
                else if (score > 30) tvState.text = "Fatigue"
            }
        }
    }
\end{lstlisting}
Deze klasse neemt een instantie van \emph{FaceBlendshapesResultBinding} als parameter. Dat is een gegenereerde binding klasse dat er voor zorgt dat er toegang is tot de bijbehorende XML layout klasses. Er wordt een \emph{double} score gebonden aan de \emph{ViewHolder}. Dit gebeurt in \emph{onBindViewHolder} en bevat het PERCLOS percentage.

\begin{lstlisting}[language=Kotlin, caption=onCreateViewHolder in FaceBlendshapesResultAdapter, label={lst:onCreateViewHolder}]
    override fun onCreateViewHolder(parent: ViewGroup, viewType: Int): ViewHolder
    {
        val binding = FaceBlendshapesResultBinding.inflate(
        LayoutInflater.from(parent.context),
        parent,
        false
        )
        return ViewHolder(binding)
    }
\end{lstlisting}
Dit zorgt er voor dat er een nieuwe \emph{ViewHolder} instantie wordt gemaakt indien de \emph{RecyclerView} een nieuwe nodig heeft. Dit is nodig zodat de data accuraat wordt weergegeven. De binding wordt opgeblazen met de layout, de parent en een \emph{boolean}. De \emph{parent} is de view waar \emph{ViewHolder} aan hangt. De \emph{boolean} die op \emph{false} staat, is een aanbevolen optie om er voor te zorgen dat deze view zich niet meteen aan de \emph{parent} hangt.

\begin{lstlisting}[language=Kotlin, caption=PERCLOS en EAR berekenen, label={lst:updateResults}]
    private var perclos = 0.0
    
    private val frameRate = 30
    private val duration = 30
    private val durationInFrames = duration * frameRate
    private var closedFrames = 0
    private var totalFrames = 0
    private val closedFramesQueue: Queue<Boolean> = LinkedList()
    private val EAR_THRESHOLD = 0.2F
    
    fun updateResults(faceLandmarkerResult: FaceLandmarkerResult? = null,
                                        inputWidth : Int, inputHeight : Int)
    {
        var leftEyeClosed = false
        var rightEyeClosed = false
        if (faceLandmarkerResult != null && faceLandmarkerResult.faceLandmarks()
                                                                    .isNotEmpty())
        {
            val faceLandmarks = faceLandmarkerResult.faceLandmarks()[0]
            val leftEyeLandmarks = listOf(
            faceLandmarks[33], faceLandmarks[160], faceLandmarks[158],
            faceLandmarks[133], faceLandmarks[153], faceLandmarks[144])
            val rightEyeLandmarks = listOf(
            faceLandmarks[263], faceLandmarks[387], faceLandmarks[385],
            faceLandmarks[362], faceLandmarks[380], faceLandmarks[373]
            )
            val pointFListLeft: MutableList<PointF> = mutableListOf()
            val pointFListRight: MutableList<PointF> = mutableListOf()
            
            for (landmark in leftEyeLandmarks) {
                pointFListLeft.add(landmarkToPointF(landmark,
                                                        inputWidth,inputHeight))
            }
            
            for (landmark in rightEyeLandmarks) {
                pointFListRight.add(landmarkToPointF(landmark,
                                                        inputWidth,inputHeight))
            }
            
            val leftEAR = calculateEAR(pointFListLeft)
            val rightEAR = calculateEAR(pointFListRight)
            
            leftEyeClosed = leftEAR < EAR_THRESHOLD
            rightEyeClosed = rightEAR < EAR_THRESHOLD
        }
        val closedEyes = leftEyeClosed && rightEyeClosed
        
        if (closedFramesQueue.size >= durationInFrames) {
            val removed = closedFramesQueue.poll()
            if (removed == true) {
                closedFrames--
            }
        }
        closedFramesQueue.add(closedEyes)
        if (closedEyes) {
            closedFrames++
        }
        val totalFrames = closedFramesQueue.size
        perclos = (closedFrames.toDouble() / totalFrames) * 100
        
        
    }
    
    fun landmarkToPointF(landmark: NormalizedLandmark, frameWidth: Int, 
                                                      frameHeight: Int): PointF
    {
        return PointF(landmark.x() * frameWidth, landmark.y() * frameHeight)
    }
    
    fun calculateDistance(p1: PointF, p2: PointF): Float {
        return sqrt((p1.x - p2.x).pow(2) + (p1.y - p2.y).pow(2))
    }
    
    fun calculateEAR(eyeLandmarks: List<PointF>): Float {
        val verticalOne = calculateDistance(eyeLandmarks[1], eyeLandmarks[5])
        val verticalTwo = calculateDistance(eyeLandmarks[2], eyeLandmarks[4])
        val horizontal = calculateDistance(eyeLandmarks[0], eyeLandmarks[3])
        
        return (verticalOne + verticalTwo) / (2.0F * horizontal)
    }
\end{lstlisting}
In de methode \emph{updateResults} wordt het resultaat van de detectie gebruikt om zo vervolgens de Eye Aspect Ratio en de PERCLOS te berekenen. De parameters bestaan uit \emph{inputWidth}, \emph{inputHeight} en {FaceLandmarkerResult}.

Allereerst worden er 2 \emph{booleans} gedeclareerd. Deze \emph{booleans} geven aan of de ogen gesloten zijn. De berekening van de openheid en geslotenheid bevindt zich in het \emph{if} statement. Die gaat eerst kijken of \emph{FaceLandmarkerResult} bestaat, er kunnen uiteraard enkel berekeningen worden uitgevoerd indien het resultaat beschikbaar is. Ook is er een controle om te kijken of er effectief wel oriëntatiepunten aanwezig zijn in het resultaat. Dit is nodig omdat we deze punten gaan gebruiken voor de EAR berekening. 

Na deze controle worden de oriëntatiepunten van het gezicht opgehaald. Dit gebeurt via \emph{faceLandmarkerResult.faceLandmarks()[0]}. Uit deze lijst van \emph{NormalizedLandmarks} worden de oriëntatiepunten van zowel het linker- als rechteroog opgehaald, die te vinden zijn in \ref{fig:eyelandmarks_mediapipe}. Die punten worden omgezet naar \emph{PointF} via \emph{landmarkToPointF}. Deze kenmerken zijn voorlopig nog genormaliseerd en hebben een waarde tussen 0 en 1. Om ze aan te passen naar de effectieve dimensies ten opzichte van het beeld moet er vermenigvuldigt worden met \emph{frameWidth} en \emph{frameHeight}. De coördinaten die we nodig hebben voor de ogen kan teruggevonden worden in \ref{fig:eyelandmarks}.

De EAR wordt berekend volgens de methode \emph{caculateEAR} voor zowel het linker- als rechteroog. Dat gebeurt volgens deze formule:
\begin{equation*}
    EAR =  \frac{||P2 - P6|| + ||P3 - P5||}{2||P1-P4||}
\end{equation*}
Deze formule heeft 3 euclidische afstanden. Die berekening gebeurt volgens de formule:
\begin{equation*}
    \(d_\text{(p,q)}}\) = \sqrt{\((p_\text{1} - q_\text{1})^{2} + (p_\text{2} - q_\text{2})^{2}\)}
\end{equation*}
Die wordt uitgevoerd in de \emph{calculateDistance} methode. Om verder de EAR van beide ogen te berekenen, worden die 3 afstanden ingesteld als variabelen van \emph{calculateEAR}. De twee verticalen kunnen gevonden worden in \ref{fig:eyelandmarks}. Dat zijn de lijnen respectievelijk tussen P2 en P6 en P3 en P5. De horizontale lijn is dan weer P1 en P4. Tenslotte wordt dit berekend door de afstanden van de twee verticalen op te tellen en te delen door tweemaal de horizontale. De EAR wordt vervolgens vergeleken met een drempelwaarde van 0.3, wat de waarde is van gesloten. Alles boven de 0.3 is dus open en anders gesloten.

Nadat de berekeningen en booleans zijn ingesteld, wordt er een nieuwe boolean gemaakt. Deze zal pas \emph{true} zijn als beide ogen onder de drempelwaarde van 0.3 zijn. Vanaf daar worden de verschillende frames bijgehouden waarin dat de ogen gesloten zijn. Er wordt een variabele gemaakt die overerft van de \emph{Queue} interface. Deze interface werkt volgens de First In First Out (FIFO) methode. Doordat de \emph{Queue} boolean krijgt als type, zal in deze variabelen enkel \emph{true} of \emph{false} zitten. Die worden dan in een \emph{LinkedList} opgeslaan.

Eerst zal er een nieuwe \emph{if} statement uitgevoerd worden. Deze statement kijkt of de grootte van de \emph{Queue} groter of gelijk staat aan de variabele \emph{durationInFrames}, wat de tijdsduur vermenigvuldigt met het aantal frames per seconde (in dit geval 30 seconden tijdsframe x 30 frames per seconde = 900). Indien de queue groter of gelijk is aan de \emph{durationInFrames} wordt het eerste element dat in die queue staat verwijderd. Dat gebeurt via \emph{poll()}. Als de waarde van dat element \emph{true} was, wat dus betekent dat de ogen gesloten waren, zal het de teller \emph{closedFrames} verminderen. Nadien zal de volgende boolean in de queue gestopt worden.
Tenslotte wordt er gecontroleerd of die boolean al dan niet op \emph{true} stond. Indien hij dat was, dan zal de teller \emph{closedFrames} met één vermeerderen.

Om als laatste stap de PERCLOS te berekenen, wordt de grootte van de queue opgeslaan in \emph{totalFrames}. Daarna wordt het aantal \emph{closedFrames} gedeeld door de \emph{totalFrames} en maal 100 gedaan om zo het percentage te verkrijgen van hoelang de ogen gesloten zijn.


